{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptions&Interactions TP-Détection d'objets\n",
    "### Alexis Lheritier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Imports et chargement d'image*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imageai'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_9631/953927674.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mimageai\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDetection\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mObjectDetection\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcv2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mexecution_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetcwd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'imageai'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from imageai.Detection import ObjectDetection\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "execution_path = os.getcwd()\n",
    "\n",
    "## Line detection import\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "# Data loading and visualization imports\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from matplotlib.pyplot import imshow, figure, subplots\n",
    "\n",
    "# Model loading\n",
    "from models.erfnet import Net as ERFNet\n",
    "from models.lcnet import Net as LCNet\n",
    "\n",
    "# utils\n",
    "from functions import color_lanes, blend\n",
    "\n",
    "# to cuda or not to cuda\n",
    "if torch.cuda.is_available():\n",
    "    map_location = lambda storage, loc: storage.cuda()\n",
    "else:\n",
    "    map_location = 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Détection d'objets dans une vidéo\n",
    "Étape 1 : Récupérer 1 image sur 10 de la vidéo.\n",
    "Pour faire cela, on va récupérer la vidéo avec opencv et ensuite stocker dans un tableau une image sur 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_9631/2922433624.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mfull_video\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcv2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mVideoCapture\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"road.mp4\"\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# On récupère la vidéo à l'aide d'opencv\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mlist_of_image\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mcpt_max\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mwhile\u001B[0m \u001B[0mfull_video\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misOpened\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mret\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfull_video\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# On récupère chaque image de la vidéo\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "full_video = cv2.VideoCapture(\"road.mp4\")  # On récupère la vidéo à l'aide d'opencv\n",
    "list_of_image = []\n",
    "cpt_max = 0\n",
    "while full_video.isOpened():\n",
    "    ret, frame = full_video.read()  # On récupère chaque image de la vidéo\n",
    "    if not ret:\n",
    "        break\n",
    "    if cpt_max % 10 == 0:\n",
    "        list_of_image.append(frame)  # On stock dans un tableau une image sur 10\n",
    "    cpt_max = cpt_max + 1\n",
    "\n",
    "full_video.release()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Étape 2 : On va définir un détecteur à partir de la classe imageai.Detection\n",
    "Charger un réseau pré-entrainé (ici, Yolo 3)\n",
    "Et enfin, le charger dans le model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "detector = ObjectDetection()\n",
    "detector.setModelTypeAsYOLOv3()\n",
    "detector.setModelPath(os.path.join(execution_path, \"pretrained-yolov3.h5\"))\n",
    "detector.loadModel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Étape 3 : Après avoir chargé le model et créé le détecteur, on va utiliser le détecteur sur chacune des frames stocké\n",
    "Les images traitées seront stocké dans un nouveau tableau"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_of_compute_image = []\n",
    "\n",
    "for i in range(len(list_of_image)):\n",
    "    image_compute = None\n",
    "    # On aurait pu augmenter le pourcentage pour détecter un objet mais celui de base permet de ne pas avoir trop d'erreur\n",
    "    # Et quand les erreurs existent, le pourcentage de concordance avec l'objet est trop élevée (environ 70%) pour que\n",
    "    # cela soit intéressant de le modifier.\n",
    "    image_compute, detections = detector.detectObjectsFromImage(input_image=list_of_image[i], input_type=\"array\",\n",
    "                                                                output_type=\"array\", minimum_percentage_probability=30)\n",
    "    if image_compute is not None:\n",
    "        list_of_compute_image.append(image_compute)\n",
    "    else:\n",
    "        print(\"ATTENTION ! Image numéro \", i, \" non traitées.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Étape 4 : Une fois les frames traitée et stockée dans la nouvelle liste, on va les reconstituer pour faire une vidéo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "height, width, _ = list_of_compute_image[0].shape\n",
    "size = (width, height)\n",
    "print(size)\n",
    "fourcc = cv2.VideoWriter_fourcc('M', 'J', 'P', 'G')\n",
    "out = cv2.VideoWriter('vehicles_detection.avi', fourcc, 10, size)\n",
    "for i in range(len(list_of_compute_image)):\n",
    "    out.write(list_of_compute_image[i])     ## On écrit dans la sortie vidéo chaque image\n",
    "\n",
    "out.release()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Combiner la détection d'objets et de lignes\n",
    "Étape 1 : On va générer à l'aide du code fournit dans le TP7 (https://github.com/fabvio/Cascade-LD) on va traiter chaque\n",
    "image pour en extraire les lignes détectées par le réseau de neurone.\n",
    "Une fois les descripteurs extrait, ils seront appliqués à l'image traitée par la détection d'objet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    map_location = lambda storage, loc: storage.cuda()\n",
    "else:\n",
    "    map_location = 'cpu'\n",
    "\n",
    "# Les descripteurs choisit sont ceux que j'ai trouvé avoir la meilleur correspondance avec les lignes existantes\n",
    "# même si des erreurs sont visibles dans la vidéo. Il aurait donc fallu entrainer le nouveau réseau avec plus d'image ou\n",
    "# avec des images de la vidéo sans pour autant faire de l'apprentissage par coeur\n",
    "\n",
    "# Descriptor size definition\n",
    "DESCRIPTOR_SIZE = 64\n",
    "# Maximum number of lanes the network has been trained with + background\n",
    "NUM_CLASSES_SEGMENTATION = 5\n",
    "# Maximum number of classes for classification\n",
    "NUM_CLASSES_CLASSIFICATION = 3\n",
    "# Image size\n",
    "HEIGHT = 360\n",
    "WIDTH = 640\n",
    "\n",
    "def extract_descriptors(label, image):\n",
    "    # avoids problems in the sampling\n",
    "    eps = 0.01\n",
    "\n",
    "    # The labels indices are not contiguous e.g. we can have index 1, 2, and 4 in an image\n",
    "    # For this reason, we should construct the descriptor array sequentially\n",
    "    inputs = torch.zeros(0, 3, DESCRIPTOR_SIZE, DESCRIPTOR_SIZE)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "\n",
    "    # This is needed to keep track of the lane we are classifying\n",
    "    mapper = {}\n",
    "    classifier_index = 0\n",
    "\n",
    "    # Iterating over all the possible lanes ids\n",
    "    for i in range(1, NUM_CLASSES_SEGMENTATION):\n",
    "        # This extracts all the points belonging to a lane with id = i\n",
    "        single_lane = label.eq(i).view(-1).nonzero().squeeze()\n",
    "\n",
    "        # As they could be not continuous, skip the ones that have no points\n",
    "        if single_lane.numel() == 0 or len(single_lane.size()) == 0:\n",
    "            continue\n",
    "\n",
    "        # Points to sample to fill a squared desciptor\n",
    "        sample = torch.zeros(DESCRIPTOR_SIZE * DESCRIPTOR_SIZE)\n",
    "        if torch.cuda.is_available():\n",
    "            sample = sample.cuda()\n",
    "\n",
    "        sample = sample.uniform_(0, single_lane.size()[0] - eps).long()\n",
    "        sample, _ = sample.sort()\n",
    "\n",
    "        # These are the points of the lane to select\n",
    "        points = torch.index_select(single_lane, 0, sample)\n",
    "\n",
    "        # First, we view the image as a set of ordered points\n",
    "        descriptor = image.squeeze().view(3, -1)\n",
    "\n",
    "        # Then we select only the previously extracted values\n",
    "        descriptor = torch.index_select(descriptor, 1, points)\n",
    "\n",
    "        # Reshape to get the actual image\n",
    "        descriptor = descriptor.view(3, DESCRIPTOR_SIZE, DESCRIPTOR_SIZE)\n",
    "        descriptor = descriptor.unsqueeze(0)\n",
    "\n",
    "        # Concatenate to get a batch that can be processed from the other network\n",
    "        inputs = torch.cat((inputs, descriptor), 0)\n",
    "\n",
    "        # Track the indices\n",
    "        mapper[classifier_index] = i\n",
    "        classifier_index += 1\n",
    "\n",
    "    return inputs, mapper\n",
    "\n",
    "\n",
    "def line_detector(image_raw, image_with_object_detection):\n",
    "    img = cv2.cvtColor(image_raw, cv2.COLOR_BGR2RGB)\n",
    "    im = Image.fromarray(img)\n",
    "\n",
    "    im = im.resize((WIDTH, HEIGHT))\n",
    "    im_tensor = ToTensor()(im)\n",
    "    im_tensor = im_tensor.unsqueeze(0)\n",
    "\n",
    "    # Creating CNNs and loading pretrained models\n",
    "    segmentation_network = ERFNet(NUM_CLASSES_SEGMENTATION)\n",
    "    classification_network = LCNet(NUM_CLASSES_CLASSIFICATION, DESCRIPTOR_SIZE, DESCRIPTOR_SIZE)\n",
    "\n",
    "    segmentation_network.load_state_dict(torch.load('pretrained/erfnet_tusimple.pth', map_location=map_location))\n",
    "    model_path = 'pretrained/classification_{}_{}class.pth'.format(DESCRIPTOR_SIZE, NUM_CLASSES_CLASSIFICATION)\n",
    "    classification_network.load_state_dict(torch.load(model_path, map_location=map_location))\n",
    "\n",
    "    segmentation_network = segmentation_network.eval()\n",
    "    classification_network = classification_network.eval()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        segmentation_network = segmentation_network.cuda()\n",
    "        classification_network = classification_network.cuda()\n",
    "\n",
    "    # Inference on instance segmentation\n",
    "    if torch.cuda.is_available():\n",
    "        im_tensor = im_tensor.cuda()\n",
    "\n",
    "    out_segmentation = segmentation_network(im_tensor)\n",
    "    out_segmentation = out_segmentation.max(dim=1)[1]\n",
    "\n",
    "    # Converting to numpy for visualization\n",
    "    out_segmentation_np = out_segmentation.cpu().numpy()[0]\n",
    "    out_segmentation_viz = np.zeros((HEIGHT, WIDTH, 3))\n",
    "\n",
    "    for i in range(1, NUM_CLASSES_SEGMENTATION):\n",
    "        rand_c1 = random.randint(1, 255)\n",
    "        rand_c2 = random.randint(1, 255)\n",
    "        rand_c3 = random.randint(1, 255)\n",
    "        out_segmentation_viz = color_lanes(\n",
    "            out_segmentation_viz, out_segmentation_np,\n",
    "            i, (rand_c1, rand_c2, rand_c3), HEIGHT, WIDTH)\n",
    "\n",
    "    # im_seg = blend(im, out_segmentation_viz)\n",
    "\n",
    "    descriptors, index_map = extract_descriptors(out_segmentation, im_tensor)\n",
    "\n",
    "    # Inference on descriptors\n",
    "    classes = classification_network(descriptors).max(1)[1]\n",
    "    print(index_map)\n",
    "    print(classes)\n",
    "\n",
    "    # Class visualization\n",
    "    out_classification_viz = np.zeros((HEIGHT, WIDTH, 3))\n",
    "\n",
    "    for i, lane_index in index_map.items():\n",
    "        if classes[i] == 0:  # Continuous blue\n",
    "            color = (255, 0, 0)\n",
    "        elif classes[i] == 1:  # Dashed green\n",
    "            color = (0, 255, 0)\n",
    "        elif classes[i] == 2:  # Double-dashed red\n",
    "            color = (0, 0, 255)\n",
    "        else:\n",
    "            raise ArithmeticError\n",
    "        image_with_object_detection[out_segmentation_np == lane_index] = color\n",
    "    # imshow(out_classification_viz.astype(int))\n",
    "    # plt.show()\n",
    "\n",
    "    return image_with_object_detection\n",
    "\n",
    "list_compute_line = []\n",
    "for i in range(len(list_of_image)):     # on va parcourir la liste des images stockés et les fournir au réseau de neurone pour détecter des lignes\n",
    "    line_compute = line_detector(list_of_image[i], list_of_compute_image[i]) # une fois les descripteurs extrait, on le dessine sur l'image déjà traité par la détection d'objet\n",
    "    list_compute_line.append(line_compute)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Étape 2 : Une fois les images traitées par chaque réseau de neurone et fusionné dans une seule image, nous utilisons\n",
    "la même méthode que précédemment pour créer une vidéo à partir des images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "height, width, _ = list_compute_line[0].shape\n",
    "size = (width, height)\n",
    "print(size)\n",
    "fourcc = cv2.VideoWriter_fourcc('M', 'J', 'P', 'G')\n",
    "out = cv2.VideoWriter('road_detection.avi', fourcc, 10, size) # on créer une sortie vidéo. La vidéo sera à 10 IPS\n",
    "for i in range(len(list_compute_line)):\n",
    "    out.write(list_compute_line[i])\n",
    "\n",
    "out.release()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (PI_TP1_Intro)",
   "language": "python",
   "name": "pycharm-b813af2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}